2022-07-31 09:47:53,484 maskrcnn_benchmark INFO: Using 1 GPUs
2022-07-31 09:47:53,485 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'VCTreePredictor', 'SOLVER.IMS_PER_BATCH', '8', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '50000', 'SOLVER.VAL_PERIOD', '2000', 'SOLVER.CHECKPOINT_PERIOD', '2000', 'GLOVE_DIR', '/home/handong/gnn-energy/Glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth', 'OUTPUT_DIR', '/home/handong/outputs/s_p_gnn', 'SOLVER.BASE_LR', '0.001', 'SAMPLER.LR', '1.0', 'SAMPLER.ITERS', '20', 'SAMPLER.VAR', '0.001', 'SAMPLER.GRAD_CLIP', '0.01', 'MODEL.DEV_RUN', 'False'], skip_test=False, slurm_id=None)
2022-07-31 09:47:53,485 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2022-07-31 09:47:57,672 maskrcnn_benchmark INFO: 
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: CentOS Linux release 7.9.2009 (Core)
GCC version: (GCC) 5.2.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 450.80.02
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.4.0
[pip3] torch-scatter==2.0.4
[pip3] torchvision==0.5.0
[conda] mkl                       2022.0.1           h06a4308_117    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] pytorch                   1.4.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.5.0                py36_cu101    pytorch
        Pillow (8.4.0)
2022-07-31 09:47:57,673 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
2022-07-31 09:47:57,673 maskrcnn_benchmark INFO: 
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
    ############### Parameters for Energy Model ##############
    EBM:
      OBJ_EMBED_DIM: 512
      REL_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5

2022-07-31 09:47:57,674 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  NUM_OBJ_CLASSES: 151
  NUM_REL_CLASSES: 51
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DTYPE: float16
ENERGY_MODEL:
  DATA_NOISE_VAR: 0.0001
  L2COEFF: 1
  LOSS: ContrastiveDivergence
  META_ARCHITECTURE: GraphEnergyModel
  TEMP: 1
  TRAINIG_MODE: joint
GLOVE_DIR: /home/handong/gnn-energy/Glove
GNN:
  DROPOUT: 0.5
  HIDDEN_SIZE: 510
  INOUT_SIZE: 510
  ITERS: 3
  NODE_DIM: 170
  NUM_HEADS: 6
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  BASE_ONLY: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  DEV_RUN: False
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    A_DROPOUT: 0.5
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EBM:
      OBJ_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
      REL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    FUSE: True
    HEAD: 8
    H_DROPOUT: 0.5
    INPUT_SIZE: 2212
    LABEL_SMOOTHING_LOSS: False
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    OUTPUT_SIZE: 4424
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: /home/handong/outputs/s_p_gnn
PATHS_CATALOG: /home/handong/gnn-energy/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/handong/gnn-energy/maskrcnn_benchmark/config/../data/datasets
SAMPLER:
  GRAD_CLIP: 0.01
  ITERS: 20
  LR: 1.0
  NAME: SGLD
  VAR: 0.001
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 8
  MAX_ITER: 50000
  MOMENTUM: 0.9
  PRE_VAL: True
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
WANDB:
  MUTE: False
2022-07-31 09:47:57,674 maskrcnn_benchmark INFO: Saving config into: /home/handong/outputs/s_p_gnn/config.yml
2022-07-31 09:47:57,697 maskrcnn_benchmark INFO: #################### prepare training ####################
2022-07-31 09:47:57,698 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building Backbone ####################
2022-07-31 09:47:58,424 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building RPN ####################
2022-07-31 09:47:58,433 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building ROI Heads ####################
2022-07-31 09:47:59,990 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 09:47:59,990 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2022-07-31 09:47:59,990 maskrcnn_benchmark.data.build INFO: Loading data statistics from: /home/handong/outputs/s_p_gnn/VG_stanford_filtered_with_attribute_train_statistics.cache
2022-07-31 09:47:59,991 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 09:48:28,990 maskrcnn_benchmark INFO: #################### end base model construction ####################
2022-07-31 09:48:29,053 maskrcnn_benchmark INFO: #################### End energy Model Constructin ####################
2022-07-31 09:48:29,332 maskrcnn_benchmark INFO: #################### end optimizer and scheduler ####################
2022-07-31 09:48:29,338 maskrcnn_benchmark INFO: #################### end distributed ####################
2022-07-31 09:48:29,340 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /home/handong/outputs/s_p_gnn/model_0026000.pth
2022-07-31 09:57:24,649 maskrcnn_benchmark INFO: Using 1 GPUs
2022-07-31 09:57:24,649 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'VCTreePredictor', 'SOLVER.IMS_PER_BATCH', '8', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '50000', 'SOLVER.VAL_PERIOD', '2000', 'SOLVER.CHECKPOINT_PERIOD', '2000', 'GLOVE_DIR', '/home/handong/gnn-energy/Glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth', 'OUTPUT_DIR', '/home/handong/outputs/s_p_gnn', 'SOLVER.BASE_LR', '0.001', 'SAMPLER.LR', '1.0', 'SAMPLER.ITERS', '20', 'SAMPLER.VAR', '0.001', 'SAMPLER.GRAD_CLIP', '0.01', 'MODEL.DEV_RUN', 'False'], skip_test=False, slurm_id=None)
2022-07-31 09:57:24,649 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2022-07-31 09:57:29,366 maskrcnn_benchmark INFO: 
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: CentOS Linux release 7.9.2009 (Core)
GCC version: (GCC) 5.2.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 450.80.02
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.4.0
[pip3] torch-scatter==2.0.4
[pip3] torchvision==0.5.0
[conda] mkl                       2022.0.1           h06a4308_117    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] pytorch                   1.4.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.5.0                py36_cu101    pytorch
        Pillow (8.4.0)
2022-07-31 09:57:29,366 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
2022-07-31 09:57:29,366 maskrcnn_benchmark INFO: 
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
    ############### Parameters for Energy Model ##############
    EBM:
      OBJ_EMBED_DIM: 512
      REL_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5

2022-07-31 09:57:29,367 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  NUM_OBJ_CLASSES: 151
  NUM_REL_CLASSES: 51
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DTYPE: float16
ENERGY_MODEL:
  DATA_NOISE_VAR: 0.0001
  L2COEFF: 1
  LOSS: ContrastiveDivergence
  META_ARCHITECTURE: GraphEnergyModel
  TEMP: 1
  TRAINIG_MODE: joint
GLOVE_DIR: /home/handong/gnn-energy/Glove
GNN:
  DROPOUT: 0.5
  HIDDEN_SIZE: 510
  INOUT_SIZE: 510
  ITERS: 3
  NODE_DIM: 170
  NUM_HEADS: 6
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  BASE_ONLY: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  DEV_RUN: False
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    A_DROPOUT: 0.5
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EBM:
      OBJ_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
      REL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    FUSE: True
    HEAD: 8
    H_DROPOUT: 0.5
    INPUT_SIZE: 2212
    LABEL_SMOOTHING_LOSS: False
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    OUTPUT_SIZE: 4424
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: /home/handong/outputs/s_p_gnn
PATHS_CATALOG: /home/handong/gnn-energy/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/handong/gnn-energy/maskrcnn_benchmark/config/../data/datasets
SAMPLER:
  GRAD_CLIP: 0.01
  ITERS: 20
  LR: 1.0
  NAME: SGLD
  VAR: 0.001
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 8
  MAX_ITER: 50000
  MOMENTUM: 0.9
  PRE_VAL: True
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
WANDB:
  MUTE: False
2022-07-31 09:57:29,368 maskrcnn_benchmark INFO: Saving config into: /home/handong/outputs/s_p_gnn/config.yml
2022-07-31 09:57:29,389 maskrcnn_benchmark INFO: #################### prepare training ####################
2022-07-31 09:57:29,390 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building Backbone ####################
2022-07-31 09:57:30,068 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building RPN ####################
2022-07-31 09:57:30,077 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building ROI Heads ####################
2022-07-31 09:57:31,623 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 09:57:31,623 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2022-07-31 09:57:31,623 maskrcnn_benchmark.data.build INFO: Loading data statistics from: /home/handong/outputs/s_p_gnn/VG_stanford_filtered_with_attribute_train_statistics.cache
2022-07-31 09:57:31,623 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 09:58:00,836 maskrcnn_benchmark INFO: #################### end base model construction ####################
2022-07-31 09:58:00,896 maskrcnn_benchmark INFO: #################### End energy Model Constructin ####################
2022-07-31 09:58:01,167 maskrcnn_benchmark INFO: #################### end optimizer and scheduler ####################
2022-07-31 09:58:01,174 maskrcnn_benchmark INFO: #################### end distributed ####################
2022-07-31 09:58:01,175 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2022-07-31 09:58:01,516 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2022-07-31 09:58:01,580 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                         loaded from roi_heads.box.feature_extractor.fc6.bias                                                                                  of shape (4096,)
2022-07-31 09:58:01,580 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                       loaded from roi_heads.box.feature_extractor.fc6.weight                                                                                of shape (4096, 12544)
2022-07-31 09:58:01,580 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                         loaded from roi_heads.box.feature_extractor.fc7.bias                                                                                  of shape (4096,)
2022-07-31 09:58:01,580 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                       loaded from roi_heads.box.feature_extractor.fc7.weight                                                                                of shape (4096, 4096)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.getheadtail.post_emb.bias of shape (8192,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.getheadtail.post_emb.weight of shape (8192, 4096)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.0.bias of shape (1024,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.0.weight of shape (1024, 4424)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.2.bias of shape (512,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.2.weight of shape (512, 1024)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.0.bias of shape (1024,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.0.weight of shape (1024, 4424)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.2.bias of shape (512,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.2.weight of shape (512, 1024)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.0.bias of shape (1024,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.0.weight of shape (1024, 4424)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.2.bias of shape (512,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.2.weight of shape (512, 1024)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_obj.bias of shape (1024,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_obj.weight of shape (1024, 4424)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                     loaded from roi_heads.box.feature_extractor.fc6.bias                                                                                  of shape (4096,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                   loaded from roi_heads.box.feature_extractor.fc6.weight                                                                                of shape (4096, 12544)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                     loaded from roi_heads.box.feature_extractor.fc7.bias                                                                                  of shape (4096,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                   loaded from roi_heads.box.feature_extractor.fc7.weight                                                                                of shape (4096, 4096)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
2022-07-31 09:58:01,581 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
2022-07-31 09:58:01,834 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2022-07-31 09:58:01,834 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 09:58:04,315 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into /home/handong/outputs/s_p_gnn/labels.json
2022-07-31 09:58:04,362 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 09:58:05,304 maskrcnn_benchmark INFO: #################### end dataloader ####################
2022-07-31 09:58:05,304 maskrcnn_benchmark INFO: Start training
2022-07-31 09:58:13,632 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: obj_emdedding.weight                              : nan, (torch.Size([512, 4096]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: obj_emdedding.bias                                : nan, (torch.Size([512]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: obj_label_embedding.weight                        : nan, (torch.Size([512, 279]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: obj_label_embedding.bias                          : nan, (torch.Size([512]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: rel_label_embedding.weight                        : nan, (torch.Size([512, 51]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: rel_label_embedding.bias                          : nan, (torch.Size([512]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: pos_embed.0.weight                                : nan, (torch.Size([32, 9]))
2022-07-31 09:58:13,634 maskrcnn_benchmark INFO: pos_embed.0.bias                                  : nan, (torch.Size([32]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: pos_embed.1.weight                                : nan, (torch.Size([32]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: pos_embed.1.bias                                  : nan, (torch.Size([32]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: pos_embed.2.weight                                : nan, (torch.Size([128, 32]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: pos_embed.2.bias                                  : nan, (torch.Size([128]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.weight                : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.weight                : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.weight                : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.weight                : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.weight                : nan, (torch.Size([512, 1024, 1, 1]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.weight                : nan, (torch.Size([512, 512, 1, 1]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 09:58:13,635 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_ih                        : nan, (torch.Size([1536]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_hh                        : nan, (torch.Size([1536]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_kernel.0.weight                     : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_kernel.0.bias                       : nan, (torch.Size([512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_kernel.2.weight                     : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_kernel.2.bias                       : nan, (torch.Size([512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_gate.bias_ih                        : nan, (torch.Size([1536]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: im_layer.node_gate.bias_hh                        : nan, (torch.Size([1536]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.bias                       : nan, (torch.Size([1]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 09:58:13,636 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.bias                       : nan, (torch.Size([1]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.weight                   : nan, (torch.Size([512, 1024]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.bias                     : nan, (torch.Size([512]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.bias                       : nan, (torch.Size([1]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.weight                   : nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.bias                     : nan, (torch.Size([512]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: energy.0.weight                                   : nan, (torch.Size([512, 1024]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: energy.0.bias                                     : nan, (torch.Size([512]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: energy.2.weight                                   : nan, (torch.Size([1, 512]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: energy.2.bias                                     : nan, (torch.Size([1]))
2022-07-31 09:58:13,637 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 09:58:13,645 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2022-07-31 09:58:13,649 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([256, 1024, 3, 3]))
2022-07-31 09:58:13,649 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,649 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([4096]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([4096]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([128, 2, 7, 7]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([128]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([256, 128, 3, 3]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([256]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
2022-07-31 09:58:13,650 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2022-07-31 09:58:13,651 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 4096]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 4424]))
2022-07-31 09:58:13,652 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 4424]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 4424]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 4424]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 4808]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 4808]))
2022-07-31 09:58:13,653 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 4808]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 4808]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2022-07-31 09:58:13,654 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_obj.weight      : 0.09839, (torch.Size([1024, 4424]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_obj.bias        : 0.00716, (torch.Size([1024]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.kernel.0.weight: 0.01230, (torch.Size([1024, 4424]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.linear_1.0.weight: 0.01222, (torch.Size([1024, 4424]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.linear_1.2.weight: 0.00745, (torch.Size([512, 1024]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.kernel.2.weight: 0.00732, (torch.Size([512, 1024]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.getheadtail.post_emb.weight: 0.00610, (torch.Size([8192, 4096]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.kernel.2.bias: 0.00509, (torch.Size([512]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.linear_1.2.bias: 0.00499, (torch.Size([512]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.kernel.0.bias: 0.00179, (torch.Size([1024]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.nodegnn.linear_1.0.bias: 0.00176, (torch.Size([1024]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: roi_heads.relation.predictor.getheadtail.post_emb.bias: 0.00047, (torch.Size([8192]))
2022-07-31 09:58:13,655 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 10:02:28,059 这等0： INFO: 有毛病
2022-07-31 10:17:00,030 maskrcnn_benchmark INFO: num: 1  eta: 3 days, 6:29:06  iter: 200  loss: 62.5836 (60.4272)  loss_rel: 0.1359 (0.1399)  loss_refine_obj: 4.0286 (4.0286)  loss_refine_roi: 0.1349 (0.1434)  loss_obj_roi: 4.0286 (4.0286)  loss_roi: 0.1443 (0.2026)  binary_loss: 54.1675 (51.9416)  ML Loss (cd): -0.0625 (-0.0574)  time: 5.5970 (5.6736)  data: 0.0199 (0.0238)  energy lr: 0.003666  base lr : 0.003666  max mem: 8256
2022-07-31 10:21:55,528 maskrcnn_benchmark INFO: Using 1 GPUs
2022-07-31 10:21:55,528 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'VCTreePredictor', 'SOLVER.IMS_PER_BATCH', '8', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '50000', 'SOLVER.VAL_PERIOD', '2000', 'SOLVER.CHECKPOINT_PERIOD', '2000', 'GLOVE_DIR', '/home/handong/gnn-energy/Glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth', 'OUTPUT_DIR', '/home/handong/outputs/s_p_gnn', 'SOLVER.BASE_LR', '0.001', 'SAMPLER.LR', '1.0', 'SAMPLER.ITERS', '20', 'SAMPLER.VAR', '0.001', 'SAMPLER.GRAD_CLIP', '0.01', 'MODEL.DEV_RUN', 'False'], skip_test=False, slurm_id=None)
2022-07-31 10:21:55,528 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2022-07-31 10:22:00,006 maskrcnn_benchmark INFO: 
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: CentOS Linux release 7.9.2009 (Core)
GCC version: (GCC) 5.2.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 450.80.02
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.4.0
[pip3] torch-scatter==2.0.4
[pip3] torchvision==0.5.0
[conda] mkl                       2022.0.1           h06a4308_117    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] pytorch                   1.4.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.5.0                py36_cu101    pytorch
        Pillow (8.4.0)
2022-07-31 10:22:00,007 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
2022-07-31 10:22:00,007 maskrcnn_benchmark INFO: 
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
    ############### Parameters for Energy Model ##############
    EBM:
      OBJ_EMBED_DIM: 512
      REL_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5

2022-07-31 10:22:00,008 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  NUM_OBJ_CLASSES: 151
  NUM_REL_CLASSES: 51
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DTYPE: float16
ENERGY_MODEL:
  DATA_NOISE_VAR: 0.0001
  L2COEFF: 1
  LOSS: ContrastiveDivergence
  META_ARCHITECTURE: GraphEnergyModel
  TEMP: 1
  TRAINIG_MODE: joint
GLOVE_DIR: /home/handong/gnn-energy/Glove
GNN:
  DROPOUT: 0.5
  HIDDEN_SIZE: 510
  INOUT_SIZE: 510
  ITERS: 3
  NODE_DIM: 170
  NUM_HEADS: 6
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  BASE_ONLY: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  DEV_RUN: False
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    A_DROPOUT: 0.5
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EBM:
      OBJ_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
      REL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    FUSE: True
    HEAD: 8
    H_DROPOUT: 0.5
    INPUT_SIZE: 2212
    LABEL_SMOOTHING_LOSS: False
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    OUTPUT_SIZE: 4424
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: /home/handong/outputs/s_p_gnn
PATHS_CATALOG: /home/handong/gnn-energy/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/handong/gnn-energy/maskrcnn_benchmark/config/../data/datasets
SAMPLER:
  GRAD_CLIP: 0.01
  ITERS: 20
  LR: 1.0
  NAME: SGLD
  VAR: 0.001
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 8
  MAX_ITER: 50000
  MOMENTUM: 0.9
  PRE_VAL: True
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
WANDB:
  MUTE: False
2022-07-31 10:22:00,009 maskrcnn_benchmark INFO: Saving config into: /home/handong/outputs/s_p_gnn/config.yml
2022-07-31 10:22:00,031 maskrcnn_benchmark INFO: #################### prepare training ####################
2022-07-31 10:22:00,033 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building Backbone ####################
2022-07-31 10:22:00,744 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building RPN ####################
2022-07-31 10:22:00,753 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building ROI Heads ####################
2022-07-31 10:22:02,376 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 10:22:02,376 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2022-07-31 10:22:02,377 maskrcnn_benchmark.data.build INFO: Loading data statistics from: /home/handong/outputs/s_p_gnn/VG_stanford_filtered_with_attribute_train_statistics.cache
2022-07-31 10:22:02,377 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 10:22:31,617 maskrcnn_benchmark INFO: #################### end base model construction ####################
2022-07-31 10:22:31,680 maskrcnn_benchmark INFO: #################### End energy Model Constructin ####################
2022-07-31 10:22:31,954 maskrcnn_benchmark INFO: #################### end optimizer and scheduler ####################
2022-07-31 10:22:31,960 maskrcnn_benchmark INFO: #################### end distributed ####################
2022-07-31 10:22:31,962 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2022-07-31 10:22:32,308 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2022-07-31 10:22:32,374 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                         loaded from roi_heads.box.feature_extractor.fc6.bias                                                                                  of shape (4096,)
2022-07-31 10:22:32,374 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                       loaded from roi_heads.box.feature_extractor.fc6.weight                                                                                of shape (4096, 12544)
2022-07-31 10:22:32,374 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                         loaded from roi_heads.box.feature_extractor.fc7.bias                                                                                  of shape (4096,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                       loaded from roi_heads.box.feature_extractor.fc7.weight                                                                                of shape (4096, 4096)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.getheadtail.post_emb.bias of shape (8192,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.getheadtail.post_emb.weight of shape (8192, 4096)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.0.bias of shape (1024,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.0.weight of shape (1024, 4424)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.2.bias of shape (512,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.kernel.2.weight of shape (512, 1024)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.0.bias of shape (1024,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.0.weight of shape (1024, 4424)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.2.bias of shape (512,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_1.2.weight of shape (512, 1024)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.0.bias of shape (1024,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.0.weight of shape (1024, 4424)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.2.bias of shape (512,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.nodegnn.linear_2.2.weight of shape (512, 1024)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_obj.bias of shape (1024,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_obj.weight of shape (1024, 4424)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                     loaded from roi_heads.box.feature_extractor.fc6.bias                                                                                  of shape (4096,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                   loaded from roi_heads.box.feature_extractor.fc6.weight                                                                                of shape (4096, 12544)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                     loaded from roi_heads.box.feature_extractor.fc7.bias                                                                                  of shape (4096,)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                   loaded from roi_heads.box.feature_extractor.fc7.weight                                                                                of shape (4096, 4096)
2022-07-31 10:22:32,375 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
2022-07-31 10:22:32,376 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
2022-07-31 10:22:32,649 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2022-07-31 10:22:32,649 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 10:22:35,268 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into /home/handong/outputs/s_p_gnn/labels.json
2022-07-31 10:22:35,318 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 10:22:36,300 maskrcnn_benchmark INFO: #################### end dataloader ####################
2022-07-31 10:22:36,301 maskrcnn_benchmark INFO: Start training
2022-07-31 10:27:43,470 maskrcnn_benchmark INFO: Using 1 GPUs
2022-07-31 10:27:43,470 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'VCTreePredictor', 'SOLVER.IMS_PER_BATCH', '8', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '50000', 'SOLVER.VAL_PERIOD', '2000', 'SOLVER.CHECKPOINT_PERIOD', '2000', 'GLOVE_DIR', '/home/handong/gnn-energy/Glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth', 'OUTPUT_DIR', '/home/handong/outputs/s_p_gnn', 'SOLVER.BASE_LR', '0.001', 'SAMPLER.LR', '1.0', 'SAMPLER.ITERS', '20', 'SAMPLER.VAR', '0.001', 'SAMPLER.GRAD_CLIP', '0.01', 'MODEL.DEV_RUN', 'False'], skip_test=False, slurm_id=None)
2022-07-31 10:27:43,470 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2022-07-31 10:27:48,051 maskrcnn_benchmark INFO: 
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: CentOS Linux release 7.9.2009 (Core)
GCC version: (GCC) 5.2.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 450.80.02
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.4.0
[pip3] torch-scatter==2.0.4
[pip3] torchvision==0.5.0
[conda] mkl                       2022.0.1           h06a4308_117    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] pytorch                   1.4.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.5.0                py36_cu101    pytorch
        Pillow (8.4.0)
2022-07-31 10:27:48,051 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
2022-07-31 10:27:48,052 maskrcnn_benchmark INFO: 
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
    ############### Parameters for Energy Model ##############
    EBM:
      OBJ_EMBED_DIM: 512
      REL_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5

2022-07-31 10:27:48,053 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  NUM_OBJ_CLASSES: 151
  NUM_REL_CLASSES: 51
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DTYPE: float16
ENERGY_MODEL:
  DATA_NOISE_VAR: 0.0001
  L2COEFF: 1
  LOSS: ContrastiveDivergence
  META_ARCHITECTURE: GraphEnergyModel
  TEMP: 1
  TRAINIG_MODE: joint
GLOVE_DIR: /home/handong/gnn-energy/Glove
GNN:
  DROPOUT: 0.5
  HIDDEN_SIZE: 510
  INOUT_SIZE: 510
  ITERS: 3
  NODE_DIM: 170
  NUM_HEADS: 6
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  BASE_ONLY: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  DEV_RUN: False
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    A_DROPOUT: 0.5
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EBM:
      OBJ_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
      REL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    FUSE: True
    HEAD: 8
    H_DROPOUT: 0.5
    INPUT_SIZE: 2212
    LABEL_SMOOTHING_LOSS: False
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    OUTPUT_SIZE: 4424
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: /home/handong/outputs/s_p_gnn
PATHS_CATALOG: /home/handong/gnn-energy/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/handong/gnn-energy/maskrcnn_benchmark/config/../data/datasets
SAMPLER:
  GRAD_CLIP: 0.01
  ITERS: 20
  LR: 1.0
  NAME: SGLD
  VAR: 0.001
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 8
  MAX_ITER: 50000
  MOMENTUM: 0.9
  PRE_VAL: True
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
WANDB:
  MUTE: False
2022-07-31 10:27:48,053 maskrcnn_benchmark INFO: Saving config into: /home/handong/outputs/s_p_gnn/config.yml
2022-07-31 10:27:48,076 maskrcnn_benchmark INFO: #################### prepare training ####################
2022-07-31 10:27:48,077 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building Backbone ####################
2022-07-31 10:27:48,786 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building RPN ####################
2022-07-31 10:27:48,795 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building ROI Heads ####################
2022-07-31 10:27:50,354 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 10:27:50,354 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2022-07-31 10:27:50,355 maskrcnn_benchmark.data.build INFO: Loading data statistics from: /home/handong/outputs/s_p_gnn/VG_stanford_filtered_with_attribute_train_statistics.cache
2022-07-31 10:27:50,355 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 10:28:19,178 maskrcnn_benchmark INFO: #################### end base model construction ####################
2022-07-31 10:28:19,239 maskrcnn_benchmark INFO: #################### End energy Model Constructin ####################
2022-07-31 10:28:19,491 maskrcnn_benchmark INFO: #################### end optimizer and scheduler ####################
2022-07-31 10:28:19,497 maskrcnn_benchmark INFO: #################### end distributed ####################
2022-07-31 10:28:19,498 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
2022-07-31 10:28:19,853 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2022-07-31 10:28:19,854 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                         loaded from roi_heads.box.feature_extractor.fc6.bias                                                                                  of shape (4096,)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                       loaded from roi_heads.box.feature_extractor.fc6.weight                                                                                of shape (4096, 12544)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                         loaded from roi_heads.box.feature_extractor.fc7.bias                                                                                  of shape (4096,)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                       loaded from roi_heads.box.feature_extractor.fc7.weight                                                                                of shape (4096, 4096)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                     loaded from roi_heads.box.feature_extractor.fc6.bias                                                                                  of shape (4096,)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                   loaded from roi_heads.box.feature_extractor.fc6.weight                                                                                of shape (4096, 12544)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                     loaded from roi_heads.box.feature_extractor.fc7.bias                                                                                  of shape (4096,)
2022-07-31 10:28:19,919 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                   loaded from roi_heads.box.feature_extractor.fc7.weight                                                                                of shape (4096, 4096)
2022-07-31 10:28:19,920 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
2022-07-31 10:28:19,920 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
2022-07-31 10:28:20,174 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2022-07-31 10:28:20,174 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 10:28:22,606 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into /home/handong/outputs/s_p_gnn/labels.json
2022-07-31 10:28:22,657 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 10:28:23,643 maskrcnn_benchmark INFO: #################### end dataloader ####################
2022-07-31 10:28:23,644 maskrcnn_benchmark INFO: Start training
2022-07-31 10:28:29,155 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2022-07-31 10:28:29,157 maskrcnn_benchmark INFO: obj_emdedding.weight                              : nan, (torch.Size([512, 4096]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: obj_emdedding.bias                                : nan, (torch.Size([512]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: obj_label_embedding.weight                        : nan, (torch.Size([512, 279]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: obj_label_embedding.bias                          : nan, (torch.Size([512]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: rel_label_embedding.weight                        : nan, (torch.Size([512, 51]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: rel_label_embedding.bias                          : nan, (torch.Size([512]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: pos_embed.0.weight                                : nan, (torch.Size([32, 9]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: pos_embed.0.bias                                  : nan, (torch.Size([32]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: pos_embed.1.weight                                : nan, (torch.Size([32]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: pos_embed.1.bias                                  : nan, (torch.Size([32]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: pos_embed.2.weight                                : nan, (torch.Size([128, 32]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: pos_embed.2.bias                                  : nan, (torch.Size([128]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.weight                : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 10:28:29,158 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.weight                : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.weight                : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.weight                : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.weight                : nan, (torch.Size([512, 1024, 1, 1]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.weight                : nan, (torch.Size([512, 512, 1, 1]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_ih                        : nan, (torch.Size([1536]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_hh                        : nan, (torch.Size([1536]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 10:28:29,159 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_kernel.0.weight                     : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_kernel.0.bias                       : nan, (torch.Size([512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_kernel.2.weight                     : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_kernel.2.bias                       : nan, (torch.Size([512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_gate.bias_ih                        : nan, (torch.Size([1536]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_layer.node_gate.bias_hh                        : nan, (torch.Size([1536]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.bias                       : nan, (torch.Size([1]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.bias                       : nan, (torch.Size([1]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.weight                   : nan, (torch.Size([512, 1024]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.bias                     : nan, (torch.Size([512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 10:28:29,160 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.bias                       : nan, (torch.Size([1]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.weight                   : nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.bias                     : nan, (torch.Size([512]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: energy.0.weight                                   : nan, (torch.Size([512, 1024]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: energy.0.bias                                     : nan, (torch.Size([512]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: energy.2.weight                                   : nan, (torch.Size([1, 512]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: energy.2.bias                                     : nan, (torch.Size([1]))
2022-07-31 10:28:29,161 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 10:28:29,168 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([256, 1024, 3, 3]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([4096]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([4096]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([128, 2, 7, 7]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([128]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([256, 128, 3, 3]))
2022-07-31 10:28:29,172 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([256]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2022-07-31 10:28:29,173 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 4096]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,174 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 4424]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 4424]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 4424]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,175 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 4424]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 4808]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 4808]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 4808]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 4808]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2022-07-31 10:28:29,176 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2022-07-31 10:28:29,177 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 10:39:56,994 maskrcnn_benchmark INFO: num: 1  eta: 1 day, 23:57:24  iter: 200  loss: 54.0034 (57.6423)  loss_rel: 0.1437 (0.1447)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 49.9217 (53.5261)  ML Loss (cd): -0.0625 (-0.0571)  time: 3.2783 (3.4667)  data: 0.0191 (0.0226)  energy lr: 0.003666  base lr : 0.003666  max mem: 7303
2022-07-31 10:51:33,542 maskrcnn_benchmark INFO: num: 2  eta: 1 day, 23:52:27  iter: 400  loss: 57.1504 (56.0083)  loss_rel: 0.1382 (0.1368)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 52.9534 (51.9027)  ML Loss (cd): -0.0625 (-0.0598)  time: 3.1719 (3.4747)  data: 0.0190 (0.0210)  energy lr: 0.006546  base lr : 0.006546  max mem: 7479
2022-07-31 11:03:09,777 maskrcnn_benchmark INFO: num: 3  eta: 1 day, 23:42:38  iter: 600  loss: 44.7940 (55.6811)  loss_rel: 0.1072 (0.1343)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 40.7046 (51.5789)  ML Loss (cd): -0.0625 (-0.0607)  time: 3.5214 (3.4769)  data: 0.0196 (0.0204)  energy lr: 0.008000  base lr : 0.008000  max mem: 7479
2022-07-31 11:14:53,688 maskrcnn_benchmark INFO: num: 4  eta: 1 day, 23:39:47  iter: 800  loss: 47.2930 (55.3188)  loss_rel: 0.1219 (0.1325)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 43.1785 (51.2189)  ML Loss (cd): -0.0625 (-0.0612)  time: 3.3341 (3.4876)  data: 0.0194 (0.0202)  energy lr: 0.008000  base lr : 0.008000  max mem: 7631
2022-07-31 11:26:21,466 maskrcnn_benchmark INFO: num: 5  eta: 1 day, 23:20:13  iter: 1000  loss: 51.7731 (55.9033)  loss_rel: 0.1272 (0.1323)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 47.7171 (51.8038)  ML Loss (cd): -0.0625 (-0.0614)  time: 3.3366 (3.4778)  data: 0.0191 (0.0199)  energy lr: 0.008000  base lr : 0.008000  max mem: 7631
2022-07-31 11:38:03,313 maskrcnn_benchmark INFO: num: 6  eta: 1 day, 23:12:53  iter: 1200  loss: 43.6485 (55.9012)  loss_rel: 0.1137 (0.1313)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 39.5401 (51.8028)  ML Loss (cd): -0.0625 (-0.0616)  time: 3.3103 (3.4831)  data: 0.0177 (0.0196)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 11:49:35,704 maskrcnn_benchmark INFO: num: 7  eta: 1 day, 22:58:50  iter: 1400  loss: 52.9456 (55.9875)  loss_rel: 0.1195 (0.1301)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 48.7992 (51.8905)  ML Loss (cd): -0.0625 (-0.0617)  time: 3.2640 (3.4800)  data: 0.0192 (0.0195)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 12:00:57,049 maskrcnn_benchmark INFO: num: 8  eta: 1 day, 22:39:50  iter: 1600  loss: 46.8284 (55.9703)  loss_rel: 0.1053 (0.1295)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 42.7593 (51.8740)  ML Loss (cd): -0.0625 (-0.0618)  time: 3.4532 (3.4709)  data: 0.0188 (0.0194)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 12:12:21,297 maskrcnn_benchmark INFO: num: 9  eta: 1 day, 22:23:50  iter: 1800  loss: 49.1091 (55.9484)  loss_rel: 0.1176 (0.1288)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 44.9916 (51.8529)  ML Loss (cd): -0.0625 (-0.0619)  time: 3.4331 (3.4654)  data: 0.0182 (0.0193)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 12:23:37,993 maskrcnn_benchmark INFO: num: 10  eta: 1 day, 22:05:44  iter: 2000  loss: 43.0971 (55.7326)  loss_rel: 0.1173 (0.1287)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 39.0401 (51.6373)  ML Loss (cd): -0.0625 (-0.0619)  time: 3.3067 (3.4572)  data: 0.0184 (0.0192)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 12:23:37,995 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /home/handong/outputs/s_p_gnn/model_0002000.pth
2022-07-31 12:23:38,852 maskrcnn_benchmark INFO: Start validating
2022-07-31 12:23:39,405 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-07-31 12:44:50,826 maskrcnn_benchmark INFO: Total run time: 0:21:11.421091 (0.25428421812057495 s / img per device, on 1 devices)
2022-07-31 12:44:50,827 maskrcnn_benchmark INFO: Model inference time: 0:20:54.131747 (0.2508263493537903 s / img per device, on 1 devices)
2022-07-31 12:48:35,839 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:   R @ 20: 0.6064;   R @ 50: 0.6591;   R @ 100: 0.6733;  for mode=predcls, type=Recall(Main).
SGG eval: ngR @ 20: 0.6939; ngR @ 50: 0.8170; ngR @ 100: 0.8871;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:  zR @ 20: 0.0111;  zR @ 50: 0.0267;  zR @ 100: 0.0511;  for mode=predcls, type=Zero Shot Recall.
SGG eval:  zR @ 20: 0.1152;  zR @ 50: 0.1801;  zR @ 100: 0.2093;  for mode=predcls, type=1 Shot Recall.
 zR @ 20: 0.1186;  zR @ 50: 0.1877;  zR @ 100: 0.2452;  for mode=predcls, type=2 Shot Recall.
 zR @ 20: 0.1344;  zR @ 50: 0.2036;  zR @ 100: 0.2402;  for mode=predcls, type=3 Shot Recall.
 zR @ 20: 0.1921;  zR @ 50: 0.2974;  zR @ 100: 0.3446;  for mode=predcls, type=4 Shot Recall.
 zR @ 20: 0.1926;  zR @ 50: 0.2963;  zR @ 100: 0.3413;  for mode=predcls, type=5 Shot Recall.
 zR @ 20: 0.2556;  zR @ 50: 0.3521;  zR @ 100: 0.3855;  for mode=predcls, type=6 Shot Recall.
 zR @ 20: 0.2080;  zR @ 50: 0.2852;  zR @ 100: 0.3319;  for mode=predcls, type=7 Shot Recall.
 zR @ 20: 0.2015;  zR @ 50: 0.2864;  zR @ 100: 0.3197;  for mode=predcls, type=8 Shot Recall.
 zR @ 20: 0.3640;  zR @ 50: 0.4317;  zR @ 100: 0.4676;  for mode=predcls, type=9 Shot Recall.
 zR @ 20: 0.2444;  zR @ 50: 0.3390;  zR @ 100: 0.3885;  for mode=predcls, type=10 Shot Recall.
 zR @ 20: 0.2234;  zR @ 50: 0.3313;  zR @ 100: 0.3632;  for mode=predcls, type=20 Shot Recall.
 zR @ 20: 0.2595;  zR @ 50: 0.3905;  zR @ 100: 0.4417;  for mode=predcls, type=25 Shot Recall.
 zR @ 20: 0.1688;  zR @ 50: 0.2208;  zR @ 100: 0.2208;  for mode=predcls, type=30 Shot Recall.
 zR @ 20: 0.4826;  zR @ 50: 0.5640;  zR @ 100: 0.5930;  for mode=predcls, type=40 Shot Recall.
 zR @ 20: 0.2917;  zR @ 50: 0.5347;  zR @ 100: 0.6131;  for mode=predcls, type=50 Shot Recall.
 zR @ 20: 0.5256;  zR @ 50: 0.6154;  zR @ 100: 0.6154;  for mode=predcls, type=100 Shot Recall.
 zR @ 20: 1.0000;  zR @ 50: 1.0000;  zR @ 100: 1.0000;  for mode=predcls, type=200 Shot Recall.
SGG eval:  mR @ 20: 0.1315;  mR @ 50: 0.1630;  mR @ 100: 0.1732;  for mode=predcls, type=Mean Recall.
(above:0.1053) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2016) (attached to:0.0092) (behind:0.5424) (belonging to:0.0000) (between:0.0000) (carrying:0.2346) (covered in:0.0119) (covering:0.0000) (eating:0.7857) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0368) (has:0.7889) (holding:0.5766) (in:0.3621) (in front of:0.1274) (laying on:0.0000) (looking at:0.0870) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4588) (of:0.4921) (on:0.8971) (on back of:0.0455) (over:0.1098) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.3557) (says:0.0000) (sitting on:0.2795) (standing on:0.0109) (to:0.0000) (under:0.2466) (using:0.2885) (walking in:0.0000) (walking on:0.1677) (watching:0.3235) (wearing:0.9735) (wears:0.0000) (with:0.1429) 
SGG eval:   A @ 20: 0.7001;   A @ 50: 0.7046;   A @ 100: 0.7046;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

2022-07-31 12:48:36,410 maskrcnn_benchmark INFO: Validation Result: 0.6733
2022-07-31 13:00:01,267 maskrcnn_benchmark INFO: num: 11  eta: 2 days, 6:54:26  iter: 2200  loss: 4.7825 (51.1008)  loss_rel: 0.1231 (0.1281)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (47.0060)  ML Loss (cd): -0.0625 (-0.0620)  time: 3.3610 (4.1353)  data: 0.0180 (0.7002)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 13:11:25,087 maskrcnn_benchmark INFO: num: 12  eta: 2 days, 5:53:18  iter: 2400  loss: 4.7744 (47.2410)  loss_rel: 0.1151 (0.1279)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (43.1466)  ML Loss (cd): -0.0625 (-0.0620)  time: 3.4076 (4.0756)  data: 0.0176 (0.6434)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 13:22:44,492 maskrcnn_benchmark INFO: num: 13  eta: 2 days, 4:58:29  iter: 2600  loss: 4.7748 (43.9752)  loss_rel: 0.1156 (0.1277)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (39.8810)  ML Loss (cd): -0.0625 (-0.0621)  time: 3.0950 (4.0234)  data: 0.0170 (0.5952)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 13:34:11,300 maskrcnn_benchmark INFO: num: 14  eta: 2 days, 4:11:57  iter: 2800  loss: 4.7794 (41.1759)  loss_rel: 0.1201 (0.1276)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (37.0818)  ML Loss (cd): -0.0625 (-0.0621)  time: 3.3539 (3.9813)  data: 0.0177 (0.5540)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 13:45:33,798 maskrcnn_benchmark INFO: num: 15  eta: 2 days, 3:28:59  iter: 3000  loss: 4.7620 (38.7501)  loss_rel: 0.1028 (0.1277)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (34.6559)  ML Loss (cd): -0.0625 (-0.0621)  time: 3.4246 (3.9434)  data: 0.0165 (0.5182)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 13:57:02,752 maskrcnn_benchmark INFO: num: 16  eta: 2 days, 2:51:31  iter: 3200  loss: 4.7821 (36.6272)  loss_rel: 0.1229 (0.1275)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (32.5333)  ML Loss (cd): -0.0625 (-0.0622)  time: 3.3019 (3.9122)  data: 0.0170 (0.4869)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 14:08:25,869 maskrcnn_benchmark INFO: num: 17  eta: 2 days, 2:15:48  iter: 3400  loss: 4.7724 (34.7540)  loss_rel: 0.1132 (0.1272)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (30.6603)  ML Loss (cd): -0.0625 (-0.0622)  time: 3.3166 (3.8830)  data: 0.0168 (0.4592)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 14:19:50,420 maskrcnn_benchmark INFO: num: 18  eta: 2 days, 1:43:05  iter: 3600  loss: 4.7747 (33.0889)  loss_rel: 0.1153 (0.1271)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (28.9955)  ML Loss (cd): -0.0625 (-0.0622)  time: 3.2673 (3.8574)  data: 0.0166 (0.4347)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 14:31:25,927 maskrcnn_benchmark INFO: num: 19  eta: 2 days, 1:14:49  iter: 3800  loss: 4.7631 (31.5990)  loss_rel: 0.1037 (0.1267)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (27.5059)  ML Loss (cd): -0.0625 (-0.0622)  time: 3.4259 (3.8374)  data: 0.0172 (0.4127)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 14:42:48,460 maskrcnn_benchmark INFO: ---Total norm 0.00253 clip coef 1978.15332-----------------
2022-07-31 14:42:48,468 maskrcnn_benchmark INFO: energy.2.bias                                     : 0.00171, (torch.Size([1]))
2022-07-31 14:42:48,468 maskrcnn_benchmark INFO: energy.0.bias                                     : 0.00120, (torch.Size([512]))
2022-07-31 14:42:48,468 maskrcnn_benchmark INFO: energy.0.weight                                   : 0.00084, (torch.Size([512, 1024]))
2022-07-31 14:42:48,468 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.bias                     : 0.00078, (torch.Size([512]))
2022-07-31 14:42:48,468 maskrcnn_benchmark INFO: energy.2.weight                                   : 0.00064, (torch.Size([1, 512]))
2022-07-31 14:42:48,468 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.bias                     : 0.00037, (torch.Size([512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.weight                   : 0.00020, (torch.Size([512, 1024]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_hh                      : 0.00020, (torch.Size([1536, 512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_ih                      : 0.00019, (torch.Size([1536, 512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.bias                  : 0.00015, (torch.Size([512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: rel_label_embedding.bias                          : 0.00012, (torch.Size([512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.weight                : 0.00006, (torch.Size([512, 512, 1, 1]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: rel_label_embedding.weight                        : 0.00005, (torch.Size([512, 51]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.weight                : 0.00004, (torch.Size([512, 1024, 1, 1]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.bias                  : 0.00004, (torch.Size([512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.weight                   : 0.00002, (torch.Size([512, 512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: obj_emdedding.weight                              : 0.00002, (torch.Size([512, 4096]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: obj_label_embedding.weight                        : 0.00002, (torch.Size([512, 279]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: im_layer.node_gate.bias_ih                        : 0.00001, (torch.Size([1536]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_ih                        : 0.00001, (torch.Size([1536]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: obj_label_embedding.bias                          : 0.00001, (torch.Size([512]))
2022-07-31 14:42:48,469 maskrcnn_benchmark INFO: im_layer.node_gate.bias_hh                        : 0.00001, (torch.Size([1536]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: im_layer.node_gate.weight_ih                      : 0.00001, (torch.Size([1536, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_hh                      : 0.00001, (torch.Size([1536, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_hh                        : 0.00000, (torch.Size([1536]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: im_layer.node_gate.weight_hh                      : 0.00000, (torch.Size([1536, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: pos_embed.2.weight                                : 0.00000, (torch.Size([128, 32]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: im_layer.node_kernel.2.bias                       : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.bias                       : 0.00000, (torch.Size([1]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_ih                      : 0.00000, (torch.Size([1536, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.weight                     : 0.00000, (torch.Size([1, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: pos_embed.0.weight                                : 0.00000, (torch.Size([32, 9]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.weight                     : 0.00000, (torch.Size([1, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: obj_emdedding.bias                                : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: im_layer.node_kernel.2.weight                     : 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.bias                  : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,470 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.bias                  : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: pos_embed.2.bias                                  : 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: im_layer.node_kernel.0.bias                       : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.weight                : 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.weight                : 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.bias                       : 0.00000, (torch.Size([1]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.bias                       : 0.00000, (torch.Size([1]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: pos_embed.1.bias                                  : 0.00000, (torch.Size([32]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.weight                : 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.bias                  : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.bias                  : 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: pos_embed.1.weight                                : 0.00000, (torch.Size([32]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.weight                : 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.weight                     : 0.00000, (torch.Size([1, 512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: im_layer.node_kernel.0.weight                     : 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: pos_embed.0.bias                                  : 0.00000, (torch.Size([32]))
2022-07-31 14:42:48,471 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 14:42:48,479 maskrcnn_benchmark INFO: ---Total norm 0.83727 clip coef 5.97177-----------------
2022-07-31 14:42:48,492 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: 0.40196, (torch.Size([4096, 4096]))
2022-07-31 14:42:48,492 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: 0.39443, (torch.Size([4096, 12544]))
2022-07-31 14:42:48,492 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 0.27967, (torch.Size([4096, 12544]))
2022-07-31 14:42:48,492 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: 0.25875, (torch.Size([256, 4808]))
2022-07-31 14:42:48,492 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: 0.25515, (torch.Size([256, 4808]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : 0.22141, (torch.Size([4096, 1024]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.18152, (torch.Size([4096, 4096]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : 0.15827, (torch.Size([51, 4096]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: 0.14317, (torch.Size([1280, 4808]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: 0.13554, (torch.Size([1536, 4808]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.11845, (torch.Size([256, 1024, 3, 3]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.06967, (torch.Size([256, 128, 3, 3]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.05381, (torch.Size([128, 2, 7, 7]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: 0.04058, (torch.Size([256, 4424]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: 0.03846, (torch.Size([256, 4424]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: 0.02095, (torch.Size([1280, 4424]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: 0.02035, (torch.Size([1536, 4424]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: 0.01830, (torch.Size([1280, 256]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : 0.01716, (torch.Size([1024, 512]))
2022-07-31 14:42:48,493 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: 0.01628, (torch.Size([1536, 256]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: 0.01184, (torch.Size([256]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: 0.01152, (torch.Size([256]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : 0.00984, (torch.Size([51]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : 0.00934, (torch.Size([4096]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.00892, (torch.Size([128]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: 0.00873, (torch.Size([22801, 51]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: 0.00866, (torch.Size([1280]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: 0.00866, (torch.Size([1280]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: 0.00769, (torch.Size([1536]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: 0.00769, (torch.Size([1536]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: 0.00769, (torch.Size([1536]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : 0.00523, (torch.Size([4096]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : 0.00436, (torch.Size([1024]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.00397, (torch.Size([151, 200]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: 0.00367, (torch.Size([1536, 256]))
2022-07-31 14:42:48,494 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.00244, (torch.Size([128]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.00225, (torch.Size([4096]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: 0.00205, (torch.Size([1536, 256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : 0.00202, (torch.Size([4096]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: 0.00200, (torch.Size([1280, 256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: 0.00194, (torch.Size([256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: 0.00187, (torch.Size([256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.00154, (torch.Size([256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.00152, (torch.Size([256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00129, (torch.Size([256]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: 0.00127, (torch.Size([128, 32]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: 0.00126, (torch.Size([1280]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: 0.00126, (torch.Size([1280]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: 0.00126, (torch.Size([1536]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: 0.00126, (torch.Size([1536]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: 0.00126, (torch.Size([1536]))
2022-07-31 14:42:48,495 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00104, (torch.Size([4096]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00100, (torch.Size([256]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: 0.00075, (torch.Size([32, 9]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00061, (torch.Size([128]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00048, (torch.Size([151, 200]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: 0.00034, (torch.Size([1536, 256]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: 0.00030, (torch.Size([128]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: 0.00014, (torch.Size([32]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: 0.00012, (torch.Size([32]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: 0.00000, (torch.Size([32]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: 0.00000, (torch.Size([128, 6]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: 0.00000, (torch.Size([128, 9]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,496 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: 0.00000, (torch.Size([1, 22801]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: 0.00000, (torch.Size([128, 4096]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: 0.00000, (torch.Size([128, 200]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: 0.00000, (torch.Size([128]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: 0.00000, (torch.Size([512, 512]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: 0.00000, (torch.Size([512]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: 0.00000, (torch.Size([1, 1537]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: 0.00000, (torch.Size([1]))
2022-07-31 14:42:48,497 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 14:42:48,502 maskrcnn_benchmark INFO: num: 20  eta: 2 days, 0:45:45  iter: 4000  loss: 4.7673 (30.2582)  loss_rel: 0.1081 (0.1266)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (26.1652)  ML Loss (cd): -0.0625 (-0.0622)  time: 3.4505 (3.8162)  data: 0.0167 (0.3929)  energy lr: 0.008000  base lr : 0.008000  max mem: 8134
2022-07-31 14:42:48,505 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /home/handong/outputs/s_p_gnn/model_0004000.pth
2022-07-31 14:42:49,274 maskrcnn_benchmark INFO: Start validating
2022-07-31 14:42:49,315 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-07-31 15:04:01,576 maskrcnn_benchmark INFO: Total run time: 0:21:12.260146 (0.25445202927589416 s / img per device, on 1 devices)
2022-07-31 15:04:01,576 maskrcnn_benchmark INFO: Model inference time: 0:20:55.098012 (0.2510196024894714 s / img per device, on 1 devices)
2022-07-31 15:06:11,029 maskrcnn_benchmark INFO: Using 1 GPUs
2022-07-31 15:06:11,029 maskrcnn_benchmark INFO: Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'VCTreePredictor', 'SOLVER.IMS_PER_BATCH', '8', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '50000', 'SOLVER.VAL_PERIOD', '2000', 'SOLVER.CHECKPOINT_PERIOD', '2000', 'GLOVE_DIR', '/home/handong/gnn-energy/Glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth', 'OUTPUT_DIR', '/home/handong/outputs/s_p_gnn', 'SOLVER.BASE_LR', '0.001', 'SAMPLER.LR', '1.0', 'SAMPLER.ITERS', '20', 'SAMPLER.VAR', '0.001', 'SAMPLER.GRAD_CLIP', '0.01', 'MODEL.DEV_RUN', 'False'], skip_test=False, slurm_id=None)
2022-07-31 15:06:11,029 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2022-07-31 15:06:15,651 maskrcnn_benchmark INFO: 
PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: CentOS Linux release 7.9.2009 (Core)
GCC version: (GCC) 5.2.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.1.168
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 450.80.02
cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.4.0
[pip3] torch-scatter==2.0.4
[pip3] torchvision==0.5.0
[conda] mkl                       2022.0.1           h06a4308_117    https://mirrors.ustc.edu.cn/anaconda/pkgs/main
[conda] pytorch                   1.4.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchvision               0.5.0                py36_cu101    pytorch
        Pillow (8.4.0)
2022-07-31 15:06:15,651 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
2022-07-31 15:06:15,651 maskrcnn_benchmark INFO: 
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
    ############### Parameters for Energy Model ##############
    EBM:
      OBJ_EMBED_DIM: 512
      REL_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5

2022-07-31 15:06:15,652 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  NUM_OBJ_CLASSES: 151
  NUM_REL_CLASSES: 51
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DTYPE: float16
ENERGY_MODEL:
  DATA_NOISE_VAR: 0.0001
  L2COEFF: 1
  LOSS: ContrastiveDivergence
  META_ARCHITECTURE: GraphEnergyModel
  TEMP: 1
  TRAINIG_MODE: joint
GLOVE_DIR: /home/handong/gnn-energy/Glove
GNN:
  DROPOUT: 0.5
  HIDDEN_SIZE: 510
  INOUT_SIZE: 510
  ITERS: 3
  NODE_DIM: 170
  NUM_HEADS: 6
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  BASE_ONLY: False
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  DEV_RUN: False
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /home/handong/energy_wights/predcls-vctree-backup/model_0016000.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    A_DROPOUT: 0.5
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EBM:
      OBJ_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
      REL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    FUSE: True
    HEAD: 8
    H_DROPOUT: 0.5
    INPUT_SIZE: 2212
    LABEL_SMOOTHING_LOSS: False
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    OUTPUT_SIZE: 4424
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: VCTreePredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: /home/handong/outputs/s_p_gnn
PATHS_CATALOG: /home/handong/gnn-energy/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /home/handong/gnn-energy/maskrcnn_benchmark/config/../data/datasets
SAMPLER:
  GRAD_CLIP: 0.01
  ITERS: 20
  LR: 1.0
  NAME: SGLD
  VAR: 0.001
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 8
  MAX_ITER: 50000
  MOMENTUM: 0.9
  PRE_VAL: True
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupReduceLROnPlateau
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
WANDB:
  MUTE: False
2022-07-31 15:06:15,653 maskrcnn_benchmark INFO: Saving config into: /home/handong/outputs/s_p_gnn/config.yml
2022-07-31 15:06:15,675 maskrcnn_benchmark INFO: #################### prepare training ####################
2022-07-31 15:06:15,676 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building Backbone ####################
2022-07-31 15:06:16,368 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building RPN ####################
2022-07-31 15:06:16,376 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building ROI Heads ####################
2022-07-31 15:06:17,910 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 15:06:17,910 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2022-07-31 15:06:17,911 maskrcnn_benchmark.data.build INFO: Loading data statistics from: /home/handong/outputs/s_p_gnn/VG_stanford_filtered_with_attribute_train_statistics.cache
2022-07-31 15:06:17,911 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2022-07-31 15:06:46,866 maskrcnn_benchmark INFO: #################### end base model construction ####################
2022-07-31 15:06:46,927 maskrcnn_benchmark INFO: #################### End energy Model Constructin ####################
2022-07-31 15:06:47,177 maskrcnn_benchmark INFO: #################### end optimizer and scheduler ####################
2022-07-31 15:06:47,183 maskrcnn_benchmark INFO: #################### end distributed ####################
2022-07-31 15:06:47,185 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /home/handong/outputs/s_p_gnn/model_0004000.pth
2022-07-31 15:06:47,938 maskrcnn_benchmark.utils.checkpoint INFO: Loading energy_optimizer from /home/handong/outputs/s_p_gnn/model_0004000.pth
2022-07-31 15:06:48,213 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2022-07-31 15:06:48,213 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 15:06:50,707 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into /home/handong/outputs/s_p_gnn/labels.json
2022-07-31 15:06:50,758 maskrcnn_benchmark.data.build WARNING: When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
2022-07-31 15:06:51,758 maskrcnn_benchmark INFO: #################### end dataloader ####################
2022-07-31 15:06:51,759 maskrcnn_benchmark INFO: Start training
2022-07-31 15:06:57,996 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: obj_emdedding.weight                              : nan, (torch.Size([512, 4096]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: obj_emdedding.bias                                : nan, (torch.Size([512]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: obj_label_embedding.weight                        : nan, (torch.Size([512, 279]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: obj_label_embedding.bias                          : nan, (torch.Size([512]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: rel_label_embedding.weight                        : nan, (torch.Size([512, 51]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: rel_label_embedding.bias                          : nan, (torch.Size([512]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: pos_embed.0.weight                                : nan, (torch.Size([32, 9]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: pos_embed.0.bias                                  : nan, (torch.Size([32]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: pos_embed.1.weight                                : nan, (torch.Size([32]))
2022-07-31 15:06:57,999 maskrcnn_benchmark INFO: pos_embed.1.bias                                  : nan, (torch.Size([32]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: pos_embed.2.weight                                : nan, (torch.Size([128, 32]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: pos_embed.2.bias                                  : nan, (torch.Size([128]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.weight                : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.weight                : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2node_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.weight                : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.weight                : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.edge2node_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.weight                : nan, (torch.Size([512, 1024, 1, 1]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.0.bias                  : nan, (torch.Size([512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.weight                : nan, (torch.Size([512, 512, 1, 1]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node2edge_kernel.2.bias                  : nan, (torch.Size([512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 15:06:58,000 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_ih                        : nan, (torch.Size([1536]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_layer.node_gate.bias_hh                        : nan, (torch.Size([1536]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_layer.edge_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_kernel.0.weight                     : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_kernel.0.bias                       : nan, (torch.Size([512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_kernel.2.weight                     : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_kernel.2.bias                       : nan, (torch.Size([512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_gate.weight_ih                      : nan, (torch.Size([1536, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_gate.weight_hh                      : nan, (torch.Size([1536, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_gate.bias_ih                        : nan, (torch.Size([1536]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: im_layer.node_gate.bias_hh                        : nan, (torch.Size([1536]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_pooler.hgate_node.0.bias                       : nan, (torch.Size([1]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_pooler.hgate_edge.0.bias                       : nan, (torch.Size([1]))
2022-07-31 15:06:58,001 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.weight                   : nan, (torch.Size([512, 1024]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: sg_pooler.poolingLayer.0.bias                     : nan, (torch.Size([512]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.weight                     : nan, (torch.Size([1, 512]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: im_pooler.hgate_node.0.bias                       : nan, (torch.Size([1]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.weight                   : nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: im_pooler.poolingLayer.0.bias                     : nan, (torch.Size([512]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: energy.0.weight                                   : nan, (torch.Size([512, 1024]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: energy.0.bias                                     : nan, (torch.Size([512]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: energy.2.weight                                   : nan, (torch.Size([1, 512]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: energy.2.bias                                     : nan, (torch.Size([1]))
2022-07-31 15:06:58,002 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 15:06:58,009 maskrcnn_benchmark INFO: ---Total norm nan clip coef nan-----------------
2022-07-31 15:06:58,012 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([256, 1024, 3, 3]))
2022-07-31 15:06:58,012 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([4096]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([4096]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([128, 2, 7, 7]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([128]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([256, 128, 3, 3]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([256]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
2022-07-31 15:06:58,013 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.weight: nan, (torch.Size([32, 9]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.0.bias: nan, (torch.Size([32]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.weight: nan, (torch.Size([32]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.1.bias: nan, (torch.Size([32]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.weight: nan, (torch.Size([128, 32]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.pos_embed.2.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.weight: nan, (torch.Size([128, 6]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.0.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.weight: nan, (torch.Size([128]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.overlap_embed.1.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.weight: nan, (torch.Size([128, 9]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.0.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,014 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.weight: nan, (torch.Size([128]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.box_embed.1.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bi_freq_prior.weight: nan, (torch.Size([1, 22801]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.weight: nan, (torch.Size([128, 4096]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_reduce.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.weight: nan, (torch.Size([128, 200]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.emb_reduce.bias: nan, (torch.Size([128]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.weight: nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_pre.bias: nan, (torch.Size([512]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.weight: nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_sub.bias: nan, (torch.Size([512]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.weight: nan, (torch.Size([512, 512]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.score_obj.bias: nan, (torch.Size([512]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.weight: nan, (torch.Size([1, 1537]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.vision_prior.bias: nan, (torch.Size([1]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 4424]))
2022-07-31 15:06:58,015 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 4424]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 4424]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 4424]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.weight: nan, (torch.Size([256, 4808]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,016 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.weight: nan, (torch.Size([1536, 4808]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffux.bias: nan, (torch.Size([1536]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.weight: nan, (torch.Size([1536, 256]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_left.bias: nan, (torch.Size([1536]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.weight: nan, (torch.Size([1536, 256]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_foreward.treeLSTM.ioffuh_right.bias: nan, (torch.Size([1536]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.weight: nan, (torch.Size([256, 4808]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.px.bias: nan, (torch.Size([256]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.weight: nan, (torch.Size([1280, 4808]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofux.bias: nan, (torch.Size([1280]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.weight: nan, (torch.Size([1280, 256]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.edge_ctx_rnn.multi_layer_lstm.0.treeLSTM_backward.treeLSTM.iofuh.bias: nan, (torch.Size([1280]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
2022-07-31 15:06:58,017 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 4096]))
2022-07-31 15:06:58,018 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
2022-07-31 15:06:58,018 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: nan, (torch.Size([22801, 51]))
2022-07-31 15:06:58,018 maskrcnn_benchmark INFO: -------------------------------
2022-07-31 15:18:27,981 maskrcnn_benchmark INFO: num: 21  eta: 1 day, 20:17:14  iter: 4200  loss: 4.7892 (4.7857)  loss_rel: 0.1299 (0.1264)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.3015 (3.4811)  data: 0.0162 (0.0200)  energy lr: 0.008000  base lr : 0.008000  max mem: 7308
2022-07-31 15:30:05,648 maskrcnn_benchmark INFO: num: 22  eta: 1 day, 20:08:23  iter: 4400  loss: 4.7736 (4.7792)  loss_rel: 0.1143 (0.1200)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.1933 (3.4847)  data: 0.0172 (0.0185)  energy lr: 0.008000  base lr : 0.008000  max mem: 7481
2022-07-31 15:41:44,409 maskrcnn_benchmark INFO: num: 23  eta: 1 day, 19:59:03  iter: 4600  loss: 4.7565 (4.7763)  loss_rel: 0.0972 (0.1170)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.4772 (3.4877)  data: 0.0175 (0.0180)  energy lr: 0.008000  base lr : 0.008000  max mem: 7481
2022-07-31 15:53:30,793 maskrcnn_benchmark INFO: num: 24  eta: 1 day, 19:55:45  iter: 4800  loss: 4.7623 (4.7744)  loss_rel: 0.1031 (0.1152)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.3434 (3.4988)  data: 0.0151 (0.0177)  energy lr: 0.008000  base lr : 0.008000  max mem: 7629
2022-07-31 16:05:01,057 maskrcnn_benchmark INFO: num: 25  eta: 1 day, 19:36:58  iter: 5000  loss: 4.7701 (4.7739)  loss_rel: 0.1107 (0.1147)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.3833 (3.4893)  data: 0.0166 (0.0175)  energy lr: 0.008000  base lr : 0.008000  max mem: 7629
2022-07-31 16:16:45,200 maskrcnn_benchmark INFO: num: 26  eta: 1 day, 19:29:15  iter: 5200  loss: 4.7601 (4.7732)  loss_rel: 0.1008 (0.1139)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.3522 (3.4945)  data: 0.0160 (0.0174)  energy lr: 0.008000  base lr : 0.008000  max mem: 8140
2022-07-31 16:28:24,248 maskrcnn_benchmark INFO: num: 27  eta: 1 day, 19:17:40  iter: 5400  loss: 4.7548 (4.7721)  loss_rel: 0.0955 (0.1129)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.3273 (3.4946)  data: 0.0162 (0.0173)  energy lr: 0.008000  base lr : 0.008000  max mem: 8140
2022-07-31 16:39:57,644 maskrcnn_benchmark INFO: num: 28  eta: 1 day, 19:03:28  iter: 5600  loss: 4.7542 (4.7716)  loss_rel: 0.0950 (0.1123)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.4436 (3.4912)  data: 0.0168 (0.0172)  energy lr: 0.008000  base lr : 0.008000  max mem: 8140
2022-07-31 16:51:31,941 maskrcnn_benchmark INFO: num: 29  eta: 1 day, 18:50:13  iter: 5800  loss: 4.7723 (4.7711)  loss_rel: 0.1130 (0.1118)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.4964 (3.4890)  data: 0.0169 (0.0171)  energy lr: 0.008000  base lr : 0.008000  max mem: 8140
2022-07-31 17:02:59,504 maskrcnn_benchmark INFO: num: 30  eta: 1 day, 18:34:50  iter: 6000  loss: 4.7668 (4.7710)  loss_rel: 0.1076 (0.1118)  loss_refine_obj: 4.0286 (4.0286)  binary_loss: 0.6931 (0.6931)  ML Loss (cd): -0.0625 (-0.0625)  time: 3.3140 (3.4839)  data: 0.0161 (0.0171)  energy lr: 0.008000  base lr : 0.008000  max mem: 8140
2022-07-31 17:02:59,507 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /home/handong/outputs/s_p_gnn/model_0006000.pth
2022-07-31 17:03:00,304 maskrcnn_benchmark INFO: Start validating
2022-07-31 17:03:00,363 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2022-07-31 17:24:21,673 maskrcnn_benchmark INFO: Total run time: 0:21:21.309181 (0.25626183614730835 s / img per device, on 1 devices)
2022-07-31 17:24:21,674 maskrcnn_benchmark INFO: Model inference time: 0:21:04.250879 (0.25285017580986024 s / img per device, on 1 devices)
2022-07-31 17:28:02,463 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:   R @ 20: 0.5972;   R @ 50: 0.6483;   R @ 100: 0.6639;  for mode=predcls, type=Recall(Main).
SGG eval: ngR @ 20: 0.6870; ngR @ 50: 0.8152; ngR @ 100: 0.8839;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:  zR @ 20: 0.0067;  zR @ 50: 0.0289;  zR @ 100: 0.0489;  for mode=predcls, type=Zero Shot Recall.
SGG eval:  zR @ 20: 0.1226;  zR @ 50: 0.2060;  zR @ 100: 0.2320;  for mode=predcls, type=1 Shot Recall.
 zR @ 20: 0.1471;  zR @ 50: 0.2177;  zR @ 100: 0.2633;  for mode=predcls, type=2 Shot Recall.
 zR @ 20: 0.1223;  zR @ 50: 0.2098;  zR @ 100: 0.2329;  for mode=predcls, type=3 Shot Recall.
 zR @ 20: 0.2015;  zR @ 50: 0.3180;  zR @ 100: 0.3502;  for mode=predcls, type=4 Shot Recall.
 zR @ 20: 0.2070;  zR @ 50: 0.3094;  zR @ 100: 0.3481;  for mode=predcls, type=5 Shot Recall.
 zR @ 20: 0.2624;  zR @ 50: 0.3607;  zR @ 100: 0.3974;  for mode=predcls, type=6 Shot Recall.
 zR @ 20: 0.2065;  zR @ 50: 0.2954;  zR @ 100: 0.3454;  for mode=predcls, type=7 Shot Recall.
 zR @ 20: 0.2313;  zR @ 50: 0.3257;  zR @ 100: 0.3811;  for mode=predcls, type=8 Shot Recall.
 zR @ 20: 0.3612;  zR @ 50: 0.4460;  zR @ 100: 0.4892;  for mode=predcls, type=9 Shot Recall.
 zR @ 20: 0.2396;  zR @ 50: 0.3423;  zR @ 100: 0.3795;  for mode=predcls, type=10 Shot Recall.
 zR @ 20: 0.2470;  zR @ 50: 0.3650;  zR @ 100: 0.3862;  for mode=predcls, type=20 Shot Recall.
 zR @ 20: 0.2810;  zR @ 50: 0.3673;  zR @ 100: 0.4226;  for mode=predcls, type=25 Shot Recall.
 zR @ 20: 0.1688;  zR @ 50: 0.1948;  zR @ 100: 0.1948;  for mode=predcls, type=30 Shot Recall.
 zR @ 20: 0.4360;  zR @ 50: 0.5349;  zR @ 100: 0.5581;  for mode=predcls, type=40 Shot Recall.
 zR @ 20: 0.2736;  zR @ 50: 0.4932;  zR @ 100: 0.5610;  for mode=predcls, type=50 Shot Recall.
 zR @ 20: 0.6410;  zR @ 50: 0.6795;  zR @ 100: 0.6795;  for mode=predcls, type=100 Shot Recall.
 zR @ 20: 1.0000;  zR @ 50: 1.0000;  zR @ 100: 1.0000;  for mode=predcls, type=200 Shot Recall.
SGG eval:  mR @ 20: 0.1293;  mR @ 50: 0.1576;  mR @ 100: 0.1676;  for mode=predcls, type=Mean Recall.
(above:0.0943) (across:0.0000) (against:0.0000) (along:0.0769) (and:0.0000) (at:0.1912) (attached to:0.0092) (behind:0.4657) (belonging to:0.0000) (between:0.0000) (carrying:0.2368) (covered in:0.0000) (covering:0.0000) (eating:0.6429) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.0074) (has:0.7249) (holding:0.5527) (in:0.3929) (in front of:0.1838) (laying on:0.0000) (looking at:0.0435) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4263) (of:0.5381) (on:0.8770) (on back of:0.0455) (over:0.1098) (painted on:0.0000) (parked on:0.0132) (part of:0.0000) (playing:0.0000) (riding:0.2812) (says:0.0000) (sitting on:0.2387) (standing on:0.0109) (to:0.0000) (under:0.2844) (using:0.1346) (walking in:0.0000) (walking on:0.2203) (watching:0.3235) (wearing:0.9620) (wears:0.0000) (with:0.2549) 
SGG eval:   A @ 20: 0.6879;   A @ 50: 0.6921;   A @ 100: 0.6921;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

2022-07-31 17:28:03,041 maskrcnn_benchmark INFO: Validation Result: 0.6639
